---
layout: post
title: 인공지능 심층학습
date: 2021-12-14 21:24 
last_modified_at: 2021-12-14 21:24
tags: [인공지능,심층학습]
categories: [인공지능,심층학습]
toc:  false
comment: true
---

### 심층학습 최적화

* 과학 혹은 공학에서의 *최적화*
    * 우주선의 최적궤도, 운영체제의 작업 할당 계획 등
* 기계학습의 *최적화*도 매우 복잡
    * 훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야 한다.  
    → <u>일반화</u> 능력이 좋아야 한다. 
* 기계 학습의 최적화가 어려운 이유 ?
    * 대리자 관계
    * 매개탐색 공간에서 목점함수의 비볼록 성질, 고차원 특징 공간, 데이터의 희소성 등
    * 긴 훈련 시간

#### 1. 목점함수: 교차 엔트로피와 로그우도  

#### 1.1 학습 과정의 성능 판정의 중요성 : 시험에서는 틀린 만큼 합당한 벌점을 받는 것이 중요하다. 그래야 다음 시험에서 틀리는 개수를 줄일 가능성이 크기 때문. 또한 틀린 개수에 상관없이 비슷한 벌점을 받는다면 성적을 올리는 데 지연이 발생할 것이다. 이러한 원리를 기계 학습에 적용  

* __평균제곱 오차__(MSE)목적함수
    * 오차가 클수록 e값이 크므로 벌점(정량적 성능)으로 활용됨
* 큰 __허점__
    * 큰 교정이 필요함에도 작은 경사도로 작게 갱신됨
    * 로지스틱 시그모이드함수의 경우 큰 값일 때 더 작은 미분값을 얻으므로 큰 벌점이 모두 전달이 안됨  
    → 해결책 : 활성함수를 ReLu로 바꾸거나 MSE목적함수를 교차 엔트로피(cross entropy)로 바꾼다. 
#### 1.2 교차 엔트로피(Cross entropy)
* 교차 엔트로피(Cross entropy)
    * 정답(label)에 해당하는 y가 확률변수 (1 or 0 을 출력하는 y)
    * 확률 분포 *p*는 정답, *Q*는 신경망(예측) 출력
    * p(0) = 1-y Q(0) = 1-o  
      p(1) = y   Q(1) = o
* 교차 엔트로피 목적함수
    * e = -(ylong2 o +(1-y) log2(1-o)), (y는 실제값 o는 예측값 )
    * 잘 수행하는지 확인
        * y = 1 , o = 0.98 (예측이 잘된 경우)  

        → 오류 e = -(1 log2 0.98 +(1-1)log2(1-0.98)) = 0.0291로 __낮은값__
        * y = 1 , 0 = 0.0001일 때 (예측이 잘못되거나 오분류된 경우)  

        → 오류 e = -(1 log2 0.0001 + (1 - 1) log2(1-0.0001)) = 13.2877로 __높은값__
    
#### 1.3 소프트맥스(Softmax) 활성함수
* 소프트맥스(Softmax)함수
    * 소프트맥스는 *최대(Max)*를 모방한다  
    
    → 출력 노드의 중간 계산결과의 최댓값을 더욱 활성화하고 다른 작은 값들은 억제  
    * 모두 더하면 1이 되어 *확률*을 모방
    * 로지스틱 시그모이드(0~1의 값)과 Max(최대값만 선택)의 합성느낌

#### 1.4 로그우도 목적함수
* 음의 로그우도(Negative log-likelihood)목적 함수 
    * e = -log2 oy
    * 모든 출력 노드값을 사용하는 __MSE__ 나 __교차 엔트로피__ 와 달리 __oy__ 라는 하나의 노드만 적용
    * oy는 샘플의 __정답__ 에 해당하는 노드의 출력값
    * 잘못 분류한 샘플은 목점함수값이 큼
    * 분류가 잘 된 샘플은 목점함수값이 작아짐 

<u> 소프트맥스와 로그우도 </u>
  
* __소프트맥스__ 는 최댓값이 아닌 값을 억제하여 0에 가깝게 만듬
* 신경망에 의한 샘플의 정답에 해당하는 노드만 보겠다는 __로그우도__ 와 잘 어울림
* 따라서 __둘을 결합__ 하여 사용

#### 2 성능향상을 위한 방법

#### 2.1 데이터 전처리
* 규모(Scale)문제
    * ex) 건강에 관련된 데이터 (키, 몸무게, 혈압)
        * 1.8m와 1.5m는 30cm 차이가 나지만 특징 값 차이는 0.30
        * 65kg과 45kg은 20.5라는 차이
        * 첫 번째와 두 번째 특징은 양수이며, 대략 70배차이 → 느린학습의 원인
* 모든 특징이 양수인 경우의 문제
    * 가중치가 뭉치로 증가 또는 감소하면 최저점을 찾아가는 경로가 느려짐

* 정규화(Normalization)로 규모와 양수의 문제를 해결
    * 특징별 독립적으로 적용
    * 통계학의 __정규 분포__ 를 활용한 표준화 변환을 적용  

    → 평균이 0이 되도록 변환 →  표준편차가 1이 되도록 변환
* 명목변수(Norminal Value)을 원핫(one-hot)코드로 변환
    *  명목변수: 객체간 서로 구분하기 위한 변수
        * ex) 성별(남(1),여(2)), 체질(태양인(1),태음인(2),소양인(3),소음인(4))
    * 명목변수는 __거리 개념__ 이 없음.
    * 원핫 코드는 값의 개수만큼 부여한다. (성별은 2비트, 체질은 4비트)

#### 2.2 가중치 초기화

* 대칭적 가중치 문제
    * 모든 값들이 대칭적으로 연결되었거나 동일한값으로 설정된 경우 중간 값이 똑같아 짐
    * 난수로 초기화함으로써 __대칭 파괴__
* 난수로 가중치 초기화
    * 가우시안(Gaussian)또는 균일(Uniform)분포에서 난수 추출, 두 분포의 성능 차이는 거의 없음
    * 난수 범위는 아주 중요하다
    * 편향은 보통 0으로 초기화
* 사례
    * AlexNet[Krizhevsky2012]: 평균 0, 표준편차 0.01인 가우시안에서 난수 생성
    * ResNet[He2016a]: 평균 0, 표춘편차 2/n(in)인 가우시안에서 난수 생성, 편향 0 

* 가중치 초기화에 따른 변화
    * 초기화가 __너무 작은 경우:__ 모든 활성 값이 0이 됨, 경사도(Gradient)도 역시 0이므로 __학습이 안됨__
    * 초기화가 __너무 큰 경우:__ 활성 값 포화, 경사도(Gradient)는 0으로 __학습이 안됨__
    * 초기화가 __적당한 경우 :__ 모든 층에서 활성 값의 분포가 좋음 __적절한 학습 수행 가능__

* 가중치 초기화 실습
    * [가중치 초기화 실습(클릭)](https://www.deeplearning.ai/ai-notes/initialization/)

#### 2.3 탄력(Momentum)

* 경사도의 잡음 현상
    * 기계 학습은 훈련집합을 이용하여 매개변수의 경사도를 추정하므로 잡음 가능성이 높다.
    * 탄력(가속도, 관성)Momentum은 경사도에 부드러움을 가하여 잡을 효과를 줄임
        * 관성(가속도): __과거__ 에 이동했던 __방식을 기억__ 하면서 기존 방향으로 일정 이상 __추가 이동함__  

        → 수렴 속도 향상(지역 최저(Local Minima), 안장점에 빠지는 문제(Saddle points) 해소)

* 관성을 적용한 가중치 갱신
    * v = 과거의 Gradient + 현재의 Gradient
    * 속도 벡터 V는 이전 경사도를 누적한 것에 해당(처음 v = 0로 출발)
    * a의 효과(관성의 정도)
        * a = 0이면 관성이 적용 안 된 이전 경사도 갱신 공식과 동일
        * a가 1에 가까울수록 이전 경사도 정보에 큰 가중치를 주는 셈 → 궤적이 매끄러움
        * 보통 0.5, 0.9 또는 0.99사용

* __관성의 효과__
    * 지나침(Overshooting)현상을 누그러뜨려 매끄럽게 만들어 줌
* 네스테로프 가속 경사도(Nestreov acceletated Gradeint)관성
    * 현재 v 값으로 다음 일동할 곳을 예견한 후, 예견한 곳의 경사도를 사용

* __학습률(learning rate)__ *p*의 중요성
    * 너무 크면 지나침(Overshooting)에 따른 진자 현상, 너무 작으면 수렴이 느림
* __적응적 학습률(Adaptive Learning Rates)__
    * 경사도에 학습률 *p*를 곱하면 기존 경사도 갱신은 모든 매개변수에 같은 크기의 학습률을 사용하는 셈
    * __적응적 학습률__ 은 __매개변수마다__ 자신의 상황에 따라 __학습률을 조절__ 해 사용
        * ex) 학습률 담금질
            * 이전 경사도와 현재 경사도의 부호가 같은 매개변수는 값을 키우고, 다른 매개변수는 값을 줄이는 전략
* __RMSProp__
    * 가중 이동 평균 기법 적용  

    r = ar + (1-a) g*g
    * a가 작을수록 최근 것에 비중을 둠
    * 보통 a로 0.9, 0.99, 0.999를 사용
* __Adam__
    * RMSProp에 관성을 추가로 적용한 알고리즘

#### 2.4 활성함수
* 신경망 회로에서, 한 노드에 대해 입력값을 다음 노드에 보낼지 말지에 대해 결정하는 함수(모델의 정확도를 올려줌)
#### 2.5 활성함수 종류
* 시그모이드(Sigmoid)함수 : 희대의 발견, 다양한 분야에서 쓰이는 함수
    * 모든 입력값에 대해 0과 1사이로 변환하는 역할을 함
    * 미분이 가능함
    * 각 층의 관계를 비선형으로 만들어 줌
    * 이진분류 문제에도 활용할 수 있다.
* Thnh함수 
    * 시그모이드 함수와 비슷
    * -1~1값을 취할 수 있다
    * 미분 가능
    * 음수값을 갖을 수 있다.
    * 0 부근에서 더 가파른 기울기를 갖는다.
* __ReLu(Rectified Linear Unit)함수__
    * 가장 많이 쓰이는 활성함수이다 ReLU(x) = max(0,x)
    * 음수값 = 0 , 양수값 = 값 그대로   
    * Leaky ReLU함수
        * 음수값도 취할 수 있는 ReLU
        * 0 이하의 작은값을 약간 고려함
    * ELU함수 
        * Leaky ReLU보다 부드러운 함수
* 소프트맥스 함수Softmax function)
    * 분류 문제에 최적화 된 함수
    * 소프트맥스의 모든 성분의 합은 항상 1이다.
    * 확률형태로 나타냄

#### 2.6 배치 정규화
* 공변량 변화(Covariate Shift)현상
    * 훈련집합과 테스트집합의 분포가 다름
    * 내부의 공변량 변화
        * 학습이 진행되면서 첫번째 층의 매개변수가 바뀜에 따라 다음 층의 분포가 바뀜
        * 초기화(가중치 초기화)를 한 첫번째 층을 제외한 다음층부터는 데이터의 분포가 수시로 바뀌는 셈
        * 층이 깊어짐에 따라 더욱 심각 → 학습을 방해하는 요인으로 작용한다.
        * 1층의 출력은 다음층(2층)에 입력이 된다.
* __배치 정규화(Batch Normalization)__
    * 공변량 시프트 현상을 누그러뜨리기 위해 __정규화를 층 단위__ 로 적용하는 기법
    * 정규화를 적용하는 곳이 중요하다(적용위치)  

    → 일반적으로 완전연결층, 합성곱층 후 혹은 비선형 함수 전 적용
    * 훈련집합 전체 또는 미니배치 중 __미니배치에 적용하는 것이 유리__  

    → 미니배치는 샘플값이 클 수록 좋기때문에 미니배치 크기가 클 수록 좋음 

* 배치 정규화 과정
    1. 미니배치 단위로 평균과 분산을 계산
    2. 구한 평균과 분산을 통해 정규화
    3. 비례와 이동 세부 조정
* 배치 정규화 장점
    * 신경망의 경사도 흐름 개선
    * 높은 학습률 허용
    * 초기화에 대한 의존성 감소
    * 드롭아웃의 필요성 감소