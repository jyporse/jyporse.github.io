---
layout: post
title: 인공지능 심층학습
date: 2021-12-014 21:24 
last_modified_at: 2021-12-14 21:24
tags: [인공지능,심층학습]
categories: [인공지능,심층학습]
toc:  false
---

### 심층학습 최적화

* 과학 혹은 공학에서의 *최적화*
    * 우주선의 최적궤도, 운영체제의 작업 할당 계획 등
* 기계학습의 *최적화*도 매우 복잡
    * 훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야 한다.  
    → <u>일반화</u> 능력이 좋아야 한다. 
* 기계 학습의 최적화가 어려운 이유 ?
    * 대리자 관계
    * 매개탐색 공간에서 목점함수의 비볼록 성질, 고차원 특징 공간, 데이터의 희소성 등
    * 긴 훈련 시간

#### 1. 목점함수: 교차 엔트로피와 로그우도  

#### 1.1 학습 과정의 성능 판정의 중요성 : 시험에서는 틀린 만큼 합당한 벌점을 받는 것이 중요하다. 그래야 다음 시험에서 틀리는 개수를 줄일 가능성이 크기 때문. 또한 틀린 개수에 상관없이 비슷한 벌점을 받는다면 성적을 올리는 데 지연이 발생할 것이다. 이러한 원리를 기계 학습에 적용  

* __평균제곱 오차__(MSE)목적함수
    * 오차가 클수록 e값이 크므로 벌점(정량적 성능)으로 활용됨
* 큰 __허점__
    * 큰 교정이 필요함에도 작은 경사도로 작게 갱신됨
    * 로지스틱 시그모이드함수의 경우 큰 값일 때 더 작은 미분값을 얻으므로 큰 벌점이 모두 전달이 안됨  
    → 해결책 : 활성함수를 ReLu로 바꾸거나 MSE목적함수를 교차 엔트로피(cross entropy)로 바꾼다. 
#### 1.2 교차 엔트로피(Cross entropy)
* 교차 엔트로피(Cross entropy)
    * 정답(label)에 해당하는 y가 확률변수 (1 or 0 을 출력하는 y)
    * 확률 분포 *p*는 정답, *Q*는 신경망(예측) 출력
    * p(0) = 1-y Q(0) = 1-o  
      p(1) = y   Q(1) = o
* 교차 엔트로피 목적함수
    * e = -(ylong2 o +(1-y) log2(1-o)), (y는 실제값 o는 예측값 )
    * 잘 수행하는지 확인
        * y = 1 , o = 0.98 (예측이 잘된 경우)  

        → 오류 e = -(1 log2 0.98 +(1-1)log2(1-0.98)) = 0.0291로 __낮은값__
        * y = 1 , 0 = 0.0001일 때 (예측이 잘못되거나 오분류된 경우)  

        → 오류 e = -(1 log2 0.0001 + (1 - 1) log2(1-0.0001)) = 13.2877로 __높은값__
    
#### 1.3 소프트맥스(Softmax) 활성함수
* 소프트맥스(Softmax)함수
    * 소프트맥스는 *최대(Max)*를 모방한다  
    
    → 출력 노드의 중간 계산결과의 최댓값을 더욱 활성화하고 다른 작은 값들은 억제  
    * 모두 더하면 1이 되어 *확률*을 모방
    * 로지스틱 시그모이드(0~1의 값)과 Max(최대값만 선택)의 합성느낌

#### 1.4 로그우도 목적함수
* 음의 로그우도(Negative log-likelihood)목적 함수 
    * e = -log2 oy
    * 모든 출력 노드값을 사용하는 __MSE__ 나 __교차 엔트로피__ 와 달리 __oy__ 라는 하나의 노드만 적용
    * oy는 샘플의 __정답__ 에 해당하는 노드의 출력값
    * 잘못 분류한 샘플은 목점함수값이 큼
    * 분류가 잘 된 샘플은 목점함수값이 작아짐 

<u> 소프트맥스와 로그우도 </u>
  
* __소프트맥스__ 는 최댓값이 아닌 값을 억제하여 0에 가깝게 만듬
* 신경망에 의한 샘플의 정답에 해당하는 노드만 보겠다는 __로그우도__ 와 잘 어울림
* 따라서 __둘을 결합__ 하여 사용